---
layout: post
title: Metis Project 2 — Delays Expected
---

Our second project for Metis began with only two constraints:
1. Gather a substantial amount of data via web scraping
2. Build a linear regression model to attempt to predict a certain target

Being an aviation enthusiast of many years, I chose to look at a common complaint of many air travelers: delay. Would there perhaps be a way to predict, with any accuracy, the degree to which one might be late on a particular flight?

![AAL 737](http://theflight.info/wp-content/gallery/american-airlines-boeing-737-800/Boeing-737-800-American-Airlines.jpg)

## Approach

I decided to focus first on arrivals at one airport (ORD) from one origin (LGA) operated by one airline (American), and expand from there if I had time. Canceled flights would not be considered. I would collect available arrival information, and calculate "lateness" by subtracting scheduled arrival from actual. This would be a simple measure of minutes, positive if late, and negative if early. Knowing the date would give me the day of the week. It was my hypothesis that some correlation might be found with lateness and both day of week and time of day.  By using these features and others to build a linear regression model, I would see how much truth this held.

## Scraping

Despite all of this information being available in a neatly-wrapped CSV file from the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/ONTIME/Index.aspx), I decided to attempt to scrape data from one of the most popular flight data sites, which shall remain unnamed throughout this discussion. Sitting on a comprehensive trove of proprietary data, this site would much rather a user purchase their data than scrape it. Thus, they don't make scraping a particualrly simple process.

As much of the data is posted via javascript, trying to read the page object generated using the requests module with Beautiful Soup is futile. By searching for a particular `class` value, one can navigate to the `div` containing the information of interest, but the `div` contains no text whatsoever.

Before giving up and changing tack, I decided to try Selenium, just in case there might be some luck to be had there. For whatever reason, the page source of the Selenium driver object did contain the desired data (in this case, all the relevant arrival times). Since each page is constructed live from the same template, it was fairly straightforward to use Beautiful Soup to extract the data from there.

Each flight number on each day had its own page, with the date, flight number, and a unique route number comprising elements of the URL. My scraping function looped through a list of flight number/route number tuples, advancing back into the year one day at a time, manufacturing a URL string to hand off to the webdriver for each day, until it was detected that there had been no flights for seven consecutive days. In this manner, I was able to collect nearly 2000 records for 17 different flight numbers for anywhere between two and seven months, depending on the flight number.

The biggest takeaways? First, scraping is an arduous process throughout which many, many things can and will go wrong. If one doesn't properly anticipate within the scraping function how to deal with all these things (e.g. how to interpret a day on which a flight didn't exist, what to do when the driver throws a timeout exception, etc.), it will not be uncommon to wake up in the morning to see that the function stalled out at 4am, halfway through the process, with none of the data saved. Which leads to the second but larger takeaway: **work in batches and save often.** In my case, I ended up putting code into the function that pickled the database every time a new flight number's flights were added. It saved a whole lot of time in the end, and would have saved even *more* time had I done it from the beginning.

At some point, I would like to devote an entire post to the scraping process and the function used to make it happen, because it took some time to get it right, and there are a few things in it that I will do every time I scrape from here on out. But for now, onward.

## Cleaning



<insert 
